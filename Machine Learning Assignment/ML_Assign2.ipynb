{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ab935b-8c13-43ab-a9ab-859017e522d1",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "### Ans:\n",
    "### Overfitting:\n",
    "Def: A model is said to be overfitting if it performs well (low bias) on training data but does not perform well (high varience) on test data. Here, the training error is low but test error is high. This is because model memorize the pattern in seen data but unable to generalize on unseen data.\\\n",
    "Consequences: \n",
    "1. negetive impact on the performence of the model in new unseen data set.\n",
    "2. model is more complex\n",
    "\n",
    "Remedy:\n",
    "1. Using k-fold cross validation.\n",
    "2. Using Lasso, Ridge regularization.\n",
    "3. Training the model with sufficient data.\n",
    "4. Using Ensemble techniques.\n",
    "\n",
    "### Underfitting:\n",
    "Def: A model is said to be underfitted on training data when it does not perform well (high bias) in training data as well as in test data (low variance). Here model cannot detect the pattern in training data. For that reason,  training error and test error both are high.\\\n",
    "Consequences: \n",
    "1. model is very simple to understand the pattern.\n",
    "2. model is effected by noise in data set.\n",
    "3. Perform a poor prediction.\n",
    "\n",
    "Remedy:\n",
    "1. Increase the model complexity.\n",
    "2. Training the model using more data.\n",
    "3. Increase number of features.\n",
    "4. Use different techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c643a-3255-4729-a4e3-654f6fccb04b",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief.\n",
    "### Ans:\n",
    "To reduce overfitting of a model, we generally use regularization in cost function, try to reduce model complexity, train the model with sampling data from training data, use cross validation, implement early stoping criterion on training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb595ec-2135-447b-9ca3-5ed155f0c5ad",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "### Ans:\n",
    "A model is said to be underfitted on training data when it does not perform well (high bias) in training data as well as in test data (low variance). Here model cannot detect the pattern in training data. For that reason, training error and test error both are high.\n",
    "### Scenarios: \n",
    "1. If we fit linear model instade of polynomial.\n",
    "2. If we cannot use some important features in training time.\n",
    "3. If we use data set with more noise and donot remove them.\n",
    "4. If we consider a wrong model for a given dta set with out knowing about the relations between the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdbd89f-3c3e-4ad0-81cd-8ebe132033cf",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "### Ans:\n",
    "**Bias:** While making predictions, a difference occurs between prediction values made by the model and actual values/expected values, and this difference is known as bias errors or Errors due to bias.\n",
    "\n",
    "**Variance:** The variance would specify the amount of variation in the prediction if the different training data was used. In simple words, variance tells that how much a random variable is different from its expected value.\n",
    "\n",
    "**Bias-Variance Trade-off:** While building the ML model, it is really important to take care of bias and variance in order to avoid overfitting and underfitting in the model. So, it is required to make a balance between bias and variance errors, and this balance between the bias error and variance error is known as the Bias-Variance trade-off.\n",
    "\n",
    "### Relations between bias and variance and their affects:\n",
    "There are possible relations:\n",
    "\n",
    "**Low-Bias, Low-Variance:**\n",
    "The combination of low bias and low variance shows an ideal machine learning model. However, it is not possible practically.\n",
    "\n",
    "**Low-Bias, High-Variance:**\n",
    "With low bias and high variance, model predictions are inconsistent and accurate on average. This case occurs when the model learns with a large number of parameters and hence leads to an overfitting.\n",
    "\n",
    "**High-Bias, Low-Variance:**\n",
    "With High bias and low variance, predictions are consistent but inaccurate on average. This case occurs when a model does not learn well with the training dataset or uses few numbers of the parameter. It leads to underfitting problems in the model.\n",
    "\n",
    "**High-Bias, High-Variance:**\n",
    "With high bias and high variance, predictions are inconsistent and also inaccurate on average. It makes very poor predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f19558-4045-40cc-9ddb-4fe92b467af1",
   "metadata": {},
   "source": [
    "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "### Ans:\n",
    "**Method for detecting overfitting:** If the training accuracy is more higher than test accuracy. \n",
    "\n",
    "**Method for detecting unerfitting:** If the training accuracy and test accuracy are very low.\n",
    "\n",
    "Compairing training accuracy and test accuracy, we can determine overfitting and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec08e0-860f-4a07-8671-2839e47ed88a",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "### Ans:\n",
    "**Bias:** While making predictions, a difference occurs between prediction values made by the model and actual values/expected values, and this difference is known as bias errors or Errors due to bias.\n",
    "\n",
    "**Variance:** The variance would specify the amount of variation in the prediction if the different training data was used. In simple words, variance tells that how much a random variable is different from its expected value.\n",
    "\n",
    "### Examples with performence:\n",
    "**High Bias:** If we fit linear curve instade of polynomial. It leads to underfitting. Model performs poorly in this situation.\n",
    "\n",
    "**High Variance** If we fit a very complex model instade of a simpler. It may leads to overfitting. Model performs poorly in this situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f0041-442e-407c-9c7e-d6941f579ca9",
   "metadata": {},
   "source": [
    "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "### Ans:\n",
    "**Regularization:** To generalize the model, we have to prevent overfitting of the model by regularization. Regularization is a technique that can be used in such a way that it will allow to maintain all variables or features in the model by reducing the magnitude of the variables. Hence, it maintains accuracy as well as a generalization of the model.\n",
    "\n",
    "**Regulatization = Loss function + Pinalty**\n",
    "\n",
    "### Common regularization techniques:\n",
    "1. Lasso Regression or L1 regularization: it helps to reduce the overfitting in the model as well as feature selection. In this technique, the cost function is altered by adding the penalty term to it. We can calculate it by multiplying with the lambda to the absolute weight of each individual feature.\n",
    "\n",
    "2. Ridge Regression or L2 regularization: it is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients. In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
    "\n",
    "3. Elastic Net: The Elastic Net Regression technique is a combination of the Ridge and Lasso regression technique. It is the linear combination of penalties for both the L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc8aff-15d5-4499-a4cc-398f6013d3af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
