{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b911301b-dc79-4ff0-8039-8dfabe4526c3",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "### Ans: \n",
    "In linear regression problems we try to find linear relations between one or more independent variables and a dependent variable.\n",
    "### Simple Linear Regression:\n",
    "If in a linear regression problem we have one independent variable and a dependent variable, then the linear regression is called simple linear regression.\n",
    "\n",
    "**Equation of simple linear regression:** $ y = \\theta_0 + \\theta_1 x $\n",
    "\n",
    "Where, y: dependent variable, x: independent variable, $\\theta_0$: y-intercept (bias), $\\theta_1$: solpe.\n",
    "#### Example:\n",
    "Let, there are 50 students in a class. Consider x(independent variable): hights of 50 students and y(dependent variable): weights of 50 students. If we want to predicts linear relation between weights and hights, then it is a simple linear regression problem.\n",
    "\n",
    "### Multiple Linear Regression:\n",
    "If in a linear regression problem we have more than one independent variables and a dependent variable, then the linear regression is called multiple linear regression.\n",
    "\n",
    "**Equation of multiple linear regression:** $ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n = \\theta_0 + \\sum_{i=1}^n \\theta_i x_i $\n",
    "\n",
    "Where, dependent variable: y, independent variables: $x_i$, weights or coefficients: $\\theta_i$, $i = 0, 1, 2, ..., n$ and $x_0 = 1$.\n",
    "\n",
    "#### Example:\n",
    "Let, there are 50 students in a class. Consider $x_1$(independent variable): hights of 50 students, $x_2$(independent variable): ages of 50 students and y(dependent variable): weights of 50 students. If we want to predicts linear relation between weights depends on hights and ages, then it is a multiple linear regression problem.\n",
    "\n",
    "### Differences:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\begin{array}{|c|c|}\n",
    "\\hline \\text { Simple Linear regression } & \\text { Multiple Linear regression } \\\\\n",
    "\\hline\n",
    "\\text {1. Number of independent variable is one.} & \\text{1. Number of independent variables is more than one.}\\\\\n",
    "\\text {2. It captures simple relations between dependent and independent variable.}  & \\text {2. It captures more complex relations dependent and independent variables.}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1f54c-e865-453c-993e-66ce464c96f0",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "### Ans:\n",
    "**For the simple and multiple regression model to hold there are some assumptions we need to make:**\n",
    "\n",
    "1. **Linear Relationship:** There is a linear relationship between the independent variable(s) and the dependent variable.\n",
    "2. **Normally Distributed Independent Variables:** All the variables are normally distributed.\n",
    "3. **No Outliers:** There are no outliers, (if there are outliers they need to be removed).\n",
    "4. **Linearly Independent:** The independent variables are all linearly independent (no variable dependents of the other variables).\n",
    "5. All the errors are independent.\n",
    "6. **Normally Distributed Errors:** The distribution of errors is normal with mean 0 and variance 1.\n",
    "7. **Homoscedascity:** The variance of errors is constant across all levels of the independent variable, this is called homoscedasticity.\n",
    "\n",
    "**Methods to check the assumptions in the given dataset:**\n",
    "1. To check linear relationship we use scatter plot.\n",
    "2. To check the normal distributed independent variables, plot a histogram of residuals.\n",
    "3. To check outliers, use a test for detecting outliers.\n",
    "4. To check independency between independent variables, we look for a correlation between each of them.\n",
    "5. To check independency of error, plot the residuals versus the time periods.\n",
    "6. To check normality of error, draw a histogram of the errors.\n",
    "7. To check homoscedascity, plot the residuals versus the predicted values of y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d5e954-3511-42f8-a535-debca16600d7",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "### Ans:\n",
    "We know, **the equation of linear regression:** $y = \\theta_0 + \\sum_{i =1} ^n \\theta_i x_i = \\theta_0 + \\theta^T X$\n",
    "\n",
    "Where, $\\theta_0$: intercept, \n",
    "$ \\theta = \\begin{bmatrix}\n",
    "          \\theta_{1} \\\\\n",
    "           \\theta_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           \\theta_{n}\n",
    "\\end{bmatrix}$ : be coefficient vector and \n",
    "$ X = \\begin{bmatrix}\n",
    "           x_{1} \\\\\n",
    "           x_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}\n",
    "         \\end{bmatrix}$ : be vector of independent variables.\n",
    "         \n",
    "         \n",
    "For simple linear regression, $y = \\theta_0 + \\theta_1 x$.\n",
    "\n",
    "Here, $\\theta_0$: intercept, it is approximate value of $y$ when, $x=0$ that is: for $x=0$, $y = \\theta_0$ \\\n",
    "and\n",
    "$\\theta_1$: slope = rate of change of $y$ per unit change of $x$.\n",
    "\n",
    "### Example:\n",
    "Suppose in house price prediction problem, $x$: area of houses and $y$: price of houses.\n",
    "\n",
    "Let, the simple linear regression equation: $y = 201 + 30x $\n",
    "\n",
    "If $x=0$, then $y = 201$, which is intercept.\n",
    "\n",
    "Slope = 30, which is rate of change of price per unit change of area of house.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6995d05-a172-47d8-9f7e-58b7d4b4de66",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "### Ans:\n",
    "First, let use define a cost function: for n data points $(x_i, y_i)$\n",
    "\n",
    "$\n",
    "J(\\theta_0, \\theta_1) = \\frac{1}{n} \\sum_{i=1}^n (y_i - h_{\\theta}(x))\n",
    "$\n",
    "\n",
    "where, $h_{\\theta}(x) = \\theta_0 + \\theta_1x$\n",
    "\n",
    "**Gradient Descent:** it is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. It is very popular optimizing algorithm in ML, which is use to reduce the error between actual values and predicted values i.e. reduce cost function.\n",
    "\n",
    "It only works on differentiable and convex function.\n",
    "\n",
    "**Convergence of GD algorithm:**\n",
    "\n",
    "Repeat until convergence\\\n",
    "{\\\n",
    "    for j = 0,1:\\\n",
    "        $\\theta_j = \\theta_j - \\alpha \\frac{\\partial J}{\\partial \\theta_j}$\\\n",
    "}\n",
    "    \n",
    "where, $\\alpha:$learning rate = 0.01(generally)\n",
    "\n",
    "**Uses in ML:**\n",
    "\n",
    "In ML, using GD algorithm, we can find the optimum point of cost function, in which the cost will be minimum.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542dcaeb-8552-48cd-8286-74624fddc1d6",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "### Ans:\n",
    "\n",
    "If in a linear regression problem we have more than one independent variables and a dependent variable, then the linear regression is called multiple linear regression.\n",
    "\n",
    "**Equation of multiple linear regression:** $ y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n = \\theta_0 + \\sum_{i=1}^n \\theta_i x_i $\n",
    "\n",
    "Where, dependent variable: y, independent variables: $x_i$, weights or coefficients: $\\theta_i$, $i = 0, 1, 2, ..., n$ and $x_0 = 1$.\n",
    "\n",
    "**Difference between multiple linear regression and simple linear regression:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\begin{array}{|c|c|}\n",
    "\\hline \\text { Simple Linear regression } & \\text { Multiple Linear regression } \\\\\n",
    "\\hline\n",
    "\\text {1. Number of independent variable is one.} & \\text{1. Number of independent variables is more than one.}\\\\\n",
    "\\text {2. It captures simple relations between dependent and independent variable.}  & \\text {2. It captures more complex relations dependent and independent variables.}\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0c998-4c94-468a-8cb7-2f890c70d5ef",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "### Ans:\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a data frame have a high correlation with one another in a regression model. This means that one independent variable can be predicted from another in a multiple linear regression model.\n",
    "\n",
    "There are two types of multicolinearity:\n",
    "1. **Structural multicollinearity:** This type occurs when we create a model term using other terms. In other words, it’s a byproduct of the model that we specify rather than being present in the data itself. For example, if you square term $X$ to model curvature, clearly there is a correlation between $X$ and $X^2$.\n",
    "2. **Data multicollinearity:** This type of multicollinearity is present in the data itself rather than being an artifact of our model. Observational experiments are more likely to exhibit this kind of multicollinearity.\n",
    "\n",
    "**Detection of multicolinearity:**\n",
    "\n",
    "There are two simple ways to indicate multicollinearity in the dataset on EDA or obtain steps using Python.\n",
    "\n",
    "1. **Variance Inflation Factor (VIF):** Variance Inflation Factor is a measure of colinearity among predictor variables within a multiple regression.\n",
    "\n",
    "    $VIF = \\frac{1}{1 - R^2}$\n",
    "    \n",
    "    where, $R^2$ value is determined to find out how well an independent variable is described by the other independent variables.\n",
    "\n",
    "2. **Heat map or correlation matrix:** it is a correlation matrix with a color gradient background. This scale will be from 0–1 with 1 being perfectly correlated:\n",
    "\n",
    "    between 0.9 and 1.0 indicates very highly correlated variables;\\\n",
    "    between 0.7 and 0.9 indicate variables that can be considered highly correlated;\\\n",
    "    between 0.5 and 0.7 indicate variables that can be considered moderately correlated;\\\n",
    "    between 0.3 and 0.5 mean low correlation.\n",
    "\n",
    "**Address of multicolinearity:**\n",
    "\n",
    "When we want to avoid highly correlated variables in our prediction we can use one of the solution:\n",
    "\n",
    "1. **Aggregate correlated features:** Feature Engineering is aggregate or combine the two highly correlated features and turn them into one variable.\n",
    "2. **Drop one:** Drop one only sounds easy but needs a proper EDA before we decide which variable we should remove.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9c6976-0d90-45e3-a543-8fd68ebe178e",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "### Ans:\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "Polynomial Regression is a regression algorithm that models the relationship between a dependent(y) and independent variable(x) as nth degree polynomial.\n",
    "\n",
    "The Polynomial Regression equation: $   y_i= b_0+b_1x_i + b_2x_i^2+ b_3x_i^3+...+ b_nx_i^n , i = 1, 2, ..., m$ \n",
    "\n",
    "Matrix form: $Y = Xb$\n",
    "\n",
    "where, $ Y = \\begin{bmatrix}\n",
    "           y_{1} \\\\\n",
    "           y_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           y_{m}\n",
    "         \\end{bmatrix}$,\n",
    "         $ X = \\begin{bmatrix}\n",
    "           1 & x_{1} & x_{1}^2 &...& x_{1}^n \\\\\n",
    "           1 & x_{2} & x_{2}^2 &...& x_{2}^n \\\\\n",
    "           \\vdots & \\vdots & \\vdots &...& \\vdots \\\\\n",
    "           1& x_{m} & x_{m}^2 &...& x_{m}^n\n",
    "         \\end{bmatrix}$ and\n",
    "         $ b = \\begin{bmatrix}\n",
    "           b_{0} \\\\\n",
    "           b_{1} \\\\\n",
    "           \\vdots \\\\\n",
    "           b_{n}\n",
    "         \\end{bmatrix}$.\n",
    "         \n",
    "         \n",
    "**Assumptions of Polynomial Regression:**\n",
    "1. The behavior of a dependent variable can be explained by a linear, or curvilinear, additive relationship between the dependent variable and a set of k independent variables $(x_i, i=1 to k)$.\n",
    "2. The relationship between the dependent variable and any independent variable is linear or curvilinear (specifically polynomial).\n",
    "3. The independent variables are independent of each other.\n",
    "4. The errors are independent, normally distributed with mean zero and a constant variance (OLS).\n",
    "         \n",
    "**Find the right degree of the equation:**\n",
    "In order to find the right degree for the model to prevent over-fitting or under-fitting, we can use:\n",
    "1. Forward Selection: This method increases the degree until it is significant enough to define the best possible model.\n",
    "2. Backward Selection: This method decreases the degree until it is significant enough to define the best possible model.\n",
    "\n",
    "         \n",
    "**Linear Regression V/S Polynomial Regression:**\n",
    "\n",
    "1. One important distinction between Linear and Polynomial Regression is that Polynomial Regression does not require a linear relationship between the independent and dependent variables in the data set. \n",
    "2. When the Linear Regression Model fails to capture the points in the data and the Linear Regression fails to adequately represent the optimum conclusion, Polynomial Regression is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4afd43-ff31-4c32-a9b8-009bd82ab346",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "\n",
    "### Ans:\n",
    "\n",
    "**Advantage of Polynomial Regression:**\n",
    "1. The best approximation of the connection between the dependent and independent variables is a polynomial. \n",
    "2. It can accommodate a wide range of functions.\n",
    "3. Polynomial is a type of curve that can accommodate a wide variety of curvatures.\n",
    "\n",
    "**Disadvantages of Polynomial Regression:**\n",
    "1. One or two outliers in the data might have a significant impact on the nonlinear analysis’ outcomes. \n",
    "2. These are overly reliant on outliers. \n",
    "3. Furthermore, there are fewer model validation methods for detecting outliers in nonlinear regression than there are for linear regression.\n",
    "\n",
    "** Situations of using polynomial regression:**\n",
    "1. Where data points are arranged in a non-linear fashion, we need the Polynomial Regression model.\n",
    "2. When the points in the data are not captured by the Linear Regression Model and the Linear Regression fails in describing the best result clearly.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59442d-d8a1-4f25-8618-9ec35c24c1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660e254-2992-49ab-97d6-64ff98f1e6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a760d-0216-4ccc-8087-3700c248405a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c920c-32cd-4a7a-9701-431897e465a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bbaa0-fea9-48c3-ab64-d4525c940f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65a12eb-c2d1-4877-a622-b8b3f80dcad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56385d0-28b7-476c-9531-34503d5163d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
